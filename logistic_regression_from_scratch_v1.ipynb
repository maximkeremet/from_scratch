{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic logistic regression implementation from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! In this notebook I tried to implement the basic [Logistic regression](#https://en.wikipedia.org/wiki/Logistic_regression)- one of the most popular and used in machine leaning area. </br> \n",
    "I am trying to understand the esential building blocks of the algorithm and dig into the math behind each of them and how it all works together.</br> \n",
    "To make it all more beginner-friendly I picked up a telecom churn dataset and perfirmed some EDA to get the grasp of the data and explain each model block in detain, trying to build an intuitive undestanding, rather than giving out complex math equasions. </br> \n",
    "\n",
    "Below is a brief outline of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Looking at the dataset.](#dataset)<br>\n",
    "  1.1. [Loading the dataset.](#loading)<br>\n",
    "  1.2. [Basic EDA.](#eda)<br>\n",
    "  1.3. [Data preprocessing.](#preprocessing)<br>  \n",
    "2. [Building the **```Logistic Regresion```** model from scratch.](#logit_our)<br> \n",
    "3. [Resources and reading.](#resources)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Looking at the dataset.\n",
    "<a id='dataset'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Loading the dataset\n",
    "<a id='loading'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just drop all data about telecom clients to see who churns or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/telecom_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Account length','Number vmail messages', 'Total day minutes','Total day calls', 'Total day charge', 'Total eve minutes',\n",
    "                      'Total eve calls', 'Total eve charge', 'Total night minutes','Total night calls', 'Total night charge', \n",
    "                      'Total intl minutes','Total intl calls', 'Total intl charge', 'Customer service calls']\n",
    "categorical_features = ['State', 'Area code', 'International plan','Voice mail plan']\n",
    "\n",
    "categorical_dummies = pd.get_dummies(data[categorical_features].astype(str))\n",
    "data_stacked = pd.concat([categorical_dummies,data[numerical_features]], axis=1)\n",
    "\n",
    "X = data_stacked.values\n",
    "y = data['Churn'].apply(lambda x: 1 if x==True else 0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Basic EDA\n",
    "<a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account length</th>\n",
       "      <th>Area code</th>\n",
       "      <th>International plan</th>\n",
       "      <th>Voice mail plan</th>\n",
       "      <th>Number vmail messages</th>\n",
       "      <th>Total day minutes</th>\n",
       "      <th>Total day calls</th>\n",
       "      <th>Total day charge</th>\n",
       "      <th>Total eve minutes</th>\n",
       "      <th>Total eve calls</th>\n",
       "      <th>Total eve charge</th>\n",
       "      <th>Total night minutes</th>\n",
       "      <th>Total night calls</th>\n",
       "      <th>Total night charge</th>\n",
       "      <th>Total intl minutes</th>\n",
       "      <th>Total intl calls</th>\n",
       "      <th>Total intl charge</th>\n",
       "      <th>Customer service calls</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Account length  Area code International plan Voice mail plan  \\\n",
       "0    KS             128        415                 No             Yes   \n",
       "1    OH             107        415                 No             Yes   \n",
       "2    NJ             137        415                 No              No   \n",
       "3    OH              84        408                Yes              No   \n",
       "4    OK              75        415                Yes              No   \n",
       "\n",
       "   Number vmail messages  Total day minutes  Total day calls  \\\n",
       "0                     25              265.1              110   \n",
       "1                     26              161.6              123   \n",
       "2                      0              243.4              114   \n",
       "3                      0              299.4               71   \n",
       "4                      0              166.7              113   \n",
       "\n",
       "   Total day charge  Total eve minutes  Total eve calls  Total eve charge  \\\n",
       "0             45.07              197.4               99             16.78   \n",
       "1             27.47              195.5              103             16.62   \n",
       "2             41.38              121.2              110             10.30   \n",
       "3             50.90               61.9               88              5.26   \n",
       "4             28.34              148.3              122             12.61   \n",
       "\n",
       "   Total night minutes  Total night calls  Total night charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "3                196.9                 89                8.86   \n",
       "4                186.9                121                8.41   \n",
       "\n",
       "   Total intl minutes  Total intl calls  Total intl charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "3                 6.6                 7               1.78   \n",
       "4                10.1                 3               2.73   \n",
       "\n",
       "   Customer service calls  Churn  \n",
       "0                       1  False  \n",
       "1                       1  False  \n",
       "2                       0  False  \n",
       "3                       2  False  \n",
       "4                       3  False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* added soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data preprocessing\n",
    "<a id='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the normalizaition by hand, using **numpy** or scale all of the numerical features, we shall ue **```StandardScaler```** module from **```sklearn.preprocessing```**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = data[numerical_features]\n",
    "X_normed = []\n",
    "\n",
    "for i in numerical.columns:\n",
    "    norm = (numerical[i]- np.mean(numerical[i])) / np.std(numerical[i])\n",
    "    X_normed.append(norm)\n",
    "    \n",
    "X_normed = np.array(X_normed).T\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting identical result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_normed, X_scaled, equal_nan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling full frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the ``Logistic Regresion`` model from scratch.\n",
    "<a id='logit_our'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So baically, it has 3 main running blocks:</br>\n",
    "- the sigmoid function;</br>\n",
    "- log loss/ likely-hood loss function;</br>\n",
    "- gradient descent component that corrects and updates the weights</br>\n",
    "\n",
    "$\\Large *$</br>\n",
    "\n",
    "$\\Large N$ of iterations</br>\n",
    "...</br>\n",
    "\n",
    "$\\LARGE PROFIT!!!$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to initialise the weights first **(1)**, </br>\n",
    "then compute the loss (error regards to the corresponding weights), **(2)** \n",
    "compute the gradient, computing partial devivative in respect to the weights and update the weights **(3)**. </br>\n",
    "That shall be done itiratively, comparing the loss of the current iteration to the loss of the previous iteration. When the loss gets the smallest the loop shall end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** So first we have to initialise the weights in zeros for the lack of best alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Compute the loss with the help of _sigmoid_ and _log loss_ functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Theta (z) = \\dfrac{1}{1 + exp^{(-1+z)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-1 * z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the log loss equasion:</br>\n",
    "\n",
    "$$ J(w) = \\bigg[\\sum_{1}^{n} (- y^{(i)}  \\log(\\phi({z^{(i)}}) - (1-y^{(i)} )  \\log(1-\\phi({z^{(i)}})\\bigg]+\\dfrac{\\lambda}{2}||w||^{2} $$\n",
    "\n",
    "Where the equasion consists of the 2 terms:</br>\n",
    "\n",
    "* $ \\sum_{i}^n - y^{(i)}  \\log(\\phi({z^{(i)}}) - (1-y^{(i)} )  \\log(1-\\phi({z^{(i)}})$ - the sum of log loss term of each $i$ value in the vertor $z$ after the sigmoid function.</br>\n",
    "</br>\n",
    "\n",
    "* $\\dfrac{\\lambda}{2}||w||^{2}$ - regularization term, which can be rewritten as following: $\\dfrac{\\lambda}{2} \\sum_{i}^n w_{j}^{2}$\n",
    "\n",
    "\n",
    "\n",
    "(the inverse to the regularization term or a **C** parameter that uses scikit-learn library.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(X,y,weights,C):\n",
    "    # product of features and weights\n",
    "    Z = np.dot(X,weights)  \n",
    "    \n",
    "    # computing loss\n",
    "    loss = (1./len(X)) * (-np.dot(y.T, np.log(sigmoid(Z))) * np.dot((1-y.T),np.log(1 - sigmoid(Z))))\n",
    "    \n",
    "    # adding regularization\n",
    "    weights_1 = weights.copy() # creating a copy of the weight vector \n",
    "    weights_1[0] = 0\n",
    "    loss += (C/(2.*len(X))) * np.dot(weights_1.T, weights)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Updating the weight via backprobagation formula:\n",
    "\n",
    "$$ \\frac{\\partial J(\\Theta)}{\\partial b} = \\frac{1}{n} \\sum_{1}^n (sigmoid(x^{(i))}-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "And run this loop for iteratevely for $N$ times!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the ```Logistic regresion``` that incorporatess all functions inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-1 * z))\n",
    "    return s\n",
    "\n",
    "def log_loss(X,y,weights,C):\n",
    "    # product of features and weights\n",
    "    Z = np.dot(X,weights)  \n",
    "    \n",
    "    # computing loss\n",
    "    loss = (1./len(X)) * (-np.dot(y.T, np.log(sigmoid(Z))) * np.dot((1-y.T),np.log(1 - sigmoid(Z))))\n",
    "    \n",
    "    # adding regularization\n",
    "    weights_1 = weights.copy() # creating a copy of the weight vector \n",
    "    weights_1[0] = 0\n",
    "    loss += (C/(2.*len(X))) * np.dot(weights_1.T, weights)\n",
    "    return loss\n",
    "\n",
    "def logistic_regression(features, target, num_steps, learning_rate, fit_intercept=True, C=1):\n",
    "    if fit_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "    \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    loss_history = []  \n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "        loss_history.append(log_loss(features, target, weights, C))\n",
    "\n",
    "        # update weights\n",
    "        output_error_signal = target - predictions\n",
    "        \n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "\n",
    "        # print the loss on every 10000th iteration\n",
    "        if step % 10000 == 0:\n",
    "            print (log_loss(features, target, weights, C))\n",
    "        \n",
    "    return loss_history, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-182.78317235\n",
      "-74.5807216477\n",
      "-74.5804844873\n",
      "-74.5802098838\n",
      "-74.5798976508\n",
      "-74.5795480252\n",
      "-74.5791612426\n",
      "-74.5787375375\n",
      "-74.5782771433\n",
      "-74.5777802921\n"
     ]
    }
   ],
   "source": [
    "loss_history, weights = logistic_regression(X_scaled, y, 100000, learning_rate = 0.0001, fit_intercept=True, C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.43476343] [[ -9.45579025e-02  -6.66006488e-02   1.81930382e-02  -8.96403779e-02\n",
      "    1.06585703e-01  -1.40706924e-02   3.75154976e-02  -9.11718540e-03\n",
      "   -8.86375246e-04  -2.35193052e-02  -1.19873635e-02  -1.23134454e-01\n",
      "   -6.12272673e-02   1.67367399e-02  -1.28141709e-01  -4.67839312e-02\n",
      "    4.38624187e-02   5.29345375e-03  -2.48299860e-02   5.54311937e-02\n",
      "    5.40104806e-02   7.84009866e-02   9.11240260e-02   6.32307190e-02\n",
      "   -2.19080938e-02   8.17832532e-02   1.55855968e-01  -2.43141553e-02\n",
      "   -8.42424178e-02  -5.94995949e-02   5.25585229e-02   1.16368095e-01\n",
      "   -3.93899871e-02   6.83744900e-02   6.28709467e-02  -1.26611954e-02\n",
      "    1.43047714e-02   2.66051067e-03   4.47413202e-02  -1.20475900e-01\n",
      "    1.34227885e-01   9.35599483e-03  -6.05754411e-02   1.28130661e-01\n",
      "    4.14260612e-02  -1.80917005e-01  -9.79555676e-02   8.99965410e-02\n",
      "   -7.36340412e-02  -3.14966345e-02  -6.88269262e-02   2.81718399e-02\n",
      "   -7.80929422e-03  -1.91565380e-02  -3.23743160e-01   3.23743160e-01\n",
      "    4.71164921e-01  -4.71164921e-01   3.79982442e-02   5.13484653e-01\n",
      "    3.30102306e-01   8.11042958e-02   3.84192560e-01   4.72466599e-01\n",
      "    2.02423701e-02  -7.84614688e-02  -1.47088644e-01   4.20531322e-03\n",
      "    3.45662682e-01  -1.96468653e+00  -2.20530217e-01   2.19807118e+00\n",
      "    7.06290272e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C=100)\n",
    "clf.fit(X_scaled, y)\n",
    "print (clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = np.dot(np.hstack((np.ones((X_scaled.shape[0], 1)),X_scaled)), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = np.dot(np.hstack((np.ones((X_scaled.shape[0], 1)),X_scaled)), weights)\n",
    "preds = np.round(sigmoid(final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scratch: 0.8679867986798679\n",
      "Accuracy from sk-learn: 0.8676867686768677\n",
      "Difference between classifiers: 0.0003000300030002734\n"
     ]
    }
   ],
   "source": [
    "scratch_acc = (preds == y).sum().astype(float) / len(preds)\n",
    "clf_acc = clf.score(X_scaled, y)\n",
    "print ('Accuracy from scratch: {0}'.format(scratch_acc))\n",
    "print ('Accuracy from sk-learn: {0}'.format(clf_acc))\n",
    "print ('Difference between classifiers: {0}'.format(abs(clf_acc-scratch_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the logictic regression we have built shows pretty much the same accuracy on buch on all given features as the default **```Logistic regresion```** from **```Sklearn```**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Resources and reading.\n",
    "<a id='resources'></a>\n",
    "\n",
    "[Detailed math representation.](#http://www.bogotobogo.com/python/scikit-learn/scikit-learn_logistic_regression.php)</br>\n",
    "[Maximmum likely-hood in detail.](#http://www.bogotobogo.com/python/scikit-learn/Maximum-Likelyhood-Estimation-MLE.php)</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
